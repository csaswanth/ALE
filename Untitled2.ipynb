{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df23c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['I', 'am', 'John','John', 'is', 'going', 'to', 'US', 'JOHN']\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad837d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I John', 'John US']\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for sentence in l:\n",
    "    output.append(\" \".join([word for word in sentence.strip().split(\" \") if not re.match(r\"[a-z]+\",word)]))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ca0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code to demonstrate working of\n",
    "# Eliminate Capital Letter Starting words from String\n",
    "# Using join() + split() + isupper()\n",
    "     \n",
    "# initializing string\n",
    "test_str = 'GeeksforGeeks is Best for Geeks'\n",
    " \n",
    "# printing original string\n",
    "print(\"The original string is : \" + str(test_str))\n",
    " \n",
    "# Eliminate Capital Letter Starting words from String\n",
    "# Using join() + split() + isupper()\n",
    "temp = test_str.split()\n",
    "res = \" \".join([ele for ele in temp if not ele[0].isupper()])\n",
    " \n",
    "# printing result\n",
    "print(\"The filtered string : \" + str(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2eb0514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import *\n",
    "t=[w for w in l if not w.isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cdd6965a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asHwanth', 'ashwantH']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "s=\"Ashwanth asHwanth NIDHI vI ShuBHam ashwantH ashwanth\"\n",
    "b=re.findall(r'\\b[a-z]+(?:[A-Z][a-z]+)+\\b|\\b[A-Z]+(?:[A-Z][a-z]+)+\\b|\\b[a-z]+(?:[A-Z][a-z]+)+\\b|\\b[a-z]+(?:[a-z][A-Z]+)+\\b|\\b[A-Z]+(?:[A-Z][a-z]+)+\\b', s)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "739f57df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ashwanth asHwanth NIDHI vI ShuBHam ashwantH ashwanth'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=text=re.sub(r\"[^\\S'\\S]\",\" \",s)#remove all special characters except \"'\"\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "226c2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "textt=\"Ashwanth is 'Data Scientist'. figre ashwanth= AsHwanth\"\n",
    "tex=[]\n",
    "def check(word):\n",
    "    cap = word[0].isupper()\n",
    "    for i,char in enumerate(word):\n",
    "        \n",
    "        if char.isupper() and not cap:\n",
    "               tex.append\n",
    "        print('detected upper case in the middle of the word at position', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f7d7831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected upper case in the middle of the word at position 0\n",
      "detected upper case in the middle of the word at position 1\n",
      "detected upper case in the middle of the word at position 2\n",
      "detected upper case in the middle of the word at position 3\n",
      "detected upper case in the middle of the word at position 4\n",
      "detected upper case in the middle of the word at position 5\n",
      "detected upper case in the middle of the word at position 6\n",
      "detected upper case in the middle of the word at position 7\n",
      "detected upper case in the middle of the word at position 8\n",
      "detected upper case in the middle of the word at position 9\n",
      "detected upper case in the middle of the word at position 10\n",
      "detected upper case in the middle of the word at position 11\n",
      "detected upper case in the middle of the word at position 12\n",
      "detected upper case in the middle of the word at position 13\n",
      "detected upper case in the middle of the word at position 14\n",
      "detected upper case in the middle of the word at position 15\n",
      "detected upper case in the middle of the word at position 16\n",
      "detected upper case in the middle of the word at position 17\n",
      "detected upper case in the middle of the word at position 18\n",
      "detected upper case in the middle of the word at position 19\n",
      "detected upper case in the middle of the word at position 20\n",
      "detected upper case in the middle of the word at position 21\n",
      "detected upper case in the middle of the word at position 22\n",
      "detected upper case in the middle of the word at position 23\n",
      "detected upper case in the middle of the word at position 24\n",
      "detected upper case in the middle of the word at position 25\n",
      "detected upper case in the middle of the word at position 26\n",
      "detected upper case in the middle of the word at position 27\n",
      "detected upper case in the middle of the word at position 28\n",
      "detected upper case in the middle of the word at position 29\n",
      "detected upper case in the middle of the word at position 30\n",
      "detected upper case in the middle of the word at position 31\n",
      "detected upper case in the middle of the word at position 32\n",
      "detected upper case in the middle of the word at position 33\n",
      "detected upper case in the middle of the word at position 34\n",
      "detected upper case in the middle of the word at position 35\n",
      "detected upper case in the middle of the word at position 36\n",
      "detected upper case in the middle of the word at position 37\n",
      "detected upper case in the middle of the word at position 38\n",
      "detected upper case in the middle of the word at position 39\n",
      "detected upper case in the middle of the word at position 40\n",
      "detected upper case in the middle of the word at position 41\n",
      "detected upper case in the middle of the word at position 42\n",
      "detected upper case in the middle of the word at position 43\n",
      "detected upper case in the middle of the word at position 44\n",
      "detected upper case in the middle of the word at position 45\n",
      "detected upper case in the middle of the word at position 46\n",
      "detected upper case in the middle of the word at position 47\n",
      "detected upper case in the middle of the word at position 48\n",
      "detected upper case in the middle of the word at position 49\n",
      "detected upper case in the middle of the word at position 50\n",
      "detected upper case in the middle of the word at position 51\n",
      "detected upper case in the middle of the word at position 52\n",
      "detected upper case in the middle of the word at position 53\n"
     ]
    }
   ],
   "source": [
    "text=check(textt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50a0d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import docx\n",
    "import re\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_lg\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "aa8dbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "textt=\"vimal, kalyan, ashwanth are goinng to lake\"\n",
    "\n",
    "def Extract_ner(ori): \n",
    "    doc = nlp(ori)\n",
    "    ori = \"\"\n",
    "    for token in doc:\n",
    "        if token.dep_ in ['nsubj|compund']:\n",
    "            new_token = \"\"\n",
    "        elif token.pos_ == \"PUNCT\":\n",
    "            new_token = token.text\n",
    "        else:\n",
    "            new_token = \" {}\".format(token.text)\n",
    "        ori += new_token\n",
    "    ori = ori[1:]\n",
    "    ori=str(ori)\n",
    "    return ori\n",
    "t=Extract_ner(textt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2e711a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ashwanth ADJ JJ amod xxxx True False\n",
      ", PUNCT , punct , False False\n",
      "vimal ADJ JJ amod xxxx True False\n",
      ", PUNCT , punct , False False\n",
      "kalyan PROPN NNP nsubj xxxx True False\n",
      "are AUX VBP aux xxx True True\n",
      "going VERB VBG ROOT xxxx True False\n",
      "to PART TO aux xx True True\n",
      "lake VERB VB xcomp xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"ashwanth, vimal, kalyan are going to lake\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,  token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e41d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "in_list='Ashwanth is @ware ho%se'\n",
    "\n",
    "char_list =['@', '%']\n",
    "Doc1 = ' '.join([ele for ele in in_list.split(' ') if all(ch not in ele for ch in char_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c425e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 2000000\n",
    "import re\n",
    "\n",
    "text=\"Ashwanth and Ash are learning python @ Cosmic Strands, But they arenn't learning java the a koRean 2,4-di-tert-butylphenol'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213a059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee6dc872",
   "metadata": {},
   "outputs": [],
   "source": [
    "char__list=['@', '#', '$']\n",
    "Stopwords = open('Stopword.txt', 'r', encoding='utf-8')\n",
    "Stopwords = Stopwords.read()\n",
    "Stopwords=Stopwords.split(\"\\n\")\n",
    "Stopwords=np.array(Stopwords)\n",
    "char_list = open('special_characters.txt', 'r', encoding='utf-8')\n",
    "char_list = char_list.read()\n",
    "char_list=char_list.split(\"\\n\")\n",
    "char_list=np.array(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21a4f2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ashwanth and Ash are learning python @ Cosmic Strands, But they arenn't learning java the a koRean 2,4-di-tert-butylphenol'\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4337fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean(text):\n",
    "    #en_us=\" \" #give path to dictionary\n",
    "    \n",
    "    #text1= re.sub('\\n', \" \", text1)\n",
    "    #text1= re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', \"\", text1)#email\n",
    "    #text1= re.sub(r'www.\\S+', \"\", text1)#url\n",
    "    #text1=re.sub(r\"http\\S+\", \"\", text1)#url\n",
    "    #text1=re.sub(r\"^fig\\S+\", \"\", text1)#figures\n",
    "    #text1=re.sub(r\"\\S+@\\S+|\\S+@\", \"\", text1)#mail ids\n",
    "    #text1=re.sub(r'\\S+.com|\\S+.edu|\\S+.org', '', text1)#domain\n",
    "    #text1=re.sub(r\"[\\d]\",\" \",text1)\n",
    "    #text1=re.sub(r\"=\",\" \",text1)\n",
    "    #proper nouns new\n",
    "    doc = nlp(text)\n",
    "    ori = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['PROPN']:\n",
    "            new_token = \"\"\n",
    "        elif token.pos_ == \"PUNCT\":\n",
    "            new_token = token.text\n",
    "        else:\n",
    "            new_token = \" {}\".format(token.text)\n",
    "        ori += new_token\n",
    "    ori = ori[1:]\n",
    "    text=str(ori)\n",
    "    \n",
    "    text = ' '.join([ele for ele in text.split(' ') if all(ch not in ele for ch in char_list)])# new code line to remove special character words\n",
    "    text=re.sub(r\"[^\\w'\\w]\",\" \",text)#remove all special characters except \"'\"\n",
    "    text=re.sub(\"'$|^'\", \"\", text)\n",
    "    text=re.sub(r'\\S+[A-Z]\\S+|\\S+[A-Z]', \" \", text)#remove words with capitals in between new code\n",
    "    #text=re.sub(r'\\b(\\s{1,2}|\\w{5,})\\b', ' ', text)#remove words greater than 3\n",
    "    text=re.sub(' +', ' ', text)\n",
    "    #text=' '.join([i for i in text.split(' ') if len(i) > 1])\n",
    "   \n",
    "    \n",
    "    \n",
    "    #to find wrong words\n",
    "    #text=text.lower()\n",
    "    #text=text.split(\" \")\n",
    "    #text=[i for i in text.lower().split(\" \") if len(i) > 2]\n",
    "    #text=[w for w in text.lower().split(\" \") if len(w)<5 and len(w)>2]\n",
    "    text= [word for word in text.lower().split(\" \") if word not in Stopwords]#remove stop words\n",
    "    #text= re.sub('\\n', \"\", text)\n",
    "    while '' in text:\n",
    "        text.remove('')\n",
    "    \n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "100e7906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"arenn't\", 'a', 'di', 'tert', 'butylphenol']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=Clean(text)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7160142f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ashwanth',\n",
       " 'learning',\n",
       " 'python',\n",
       " 'Cosmic',\n",
       " 'Strands,',\n",
       " \"arenn't\",\n",
       " 'learning',\n",
       " 'koRean',\n",
       " \"ashu'\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=[w for w in text.split() if len(w)>4]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16a0ac22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e=[i for i in d if len(i) > 5]\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2bbd0861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ashwanth and Ash are learning python @ Cosmic Strands, But they arenn't learning java the a koRean 2,4-di-tert-butylphenol 'ashwanth\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Ashwanth and Ash are learning python @ Cosmic Strands, But they arenn't learning java the a koRean 2,4-di-tert-butylphenol 'ashwanth'\"\n",
    "f=re.sub(\"'$\", \"\", text)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54035c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spellings",
   "language": "python",
   "name": "spellings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
