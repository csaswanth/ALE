{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95f85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import docx\n",
    "mydoc=Document()\n",
    "#def get_data_from_word(path):\n",
    "    #doc_object=open(path,'rb')\n",
    "    #doc_reader=Document(doc_object)\n",
    "    #data=\" \"\n",
    "    #for p in doc_reader.paragraphs:\n",
    "        #data+=p.text+ '\\n'\n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac183bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_file=r\"C:/Users/Shubham/Desktop/Data Scientist.docx\"\n",
    "#text=get_data_from_word(path_to_file)\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918b85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#sentences=nltk.sent_tokenize(text)\n",
    "#sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a16a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/22/2022 14:12:54 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText,TTSettings\n",
    "happy=HappyTextToText(\"T5\",'t5-base',r\"C:\\Users\\Shubham\\Downloads\\gm_model-20220609T071856Z-001\\gm_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dd94d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_settings=TTSettings(num_beams=3,min_length=0,max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e82230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_={}\n",
    "#import nltk\n",
    "#for table in doc.tables:\n",
    "    #for row in table.rows:\n",
    "        #for cell in row.cells:\n",
    "            #for paragraph in cell.paragraphs:\n",
    "                #data=nltk.sent_tokenize(paragraph.text)\n",
    "                #for sent in data:\n",
    "                    #result_1=happy.generate_text(\"grammar: \"+sent,args=beam_settings)\n",
    "                    #dict_.update({sent:result_1.text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "704cb91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9474bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for table in doc.tables:\n",
    "    #for row in table.rows:\n",
    "        #for cell in row.cells:\n",
    "            #for paragraph in cell.paragraphs:\n",
    "                #inline=paragraph.runs\n",
    "                #for i in range(len(inline)):\n",
    "                    #text=inline[i].text\n",
    "                    #for key,value in dict_.items():\n",
    "                        #if key in text:\n",
    "                            #text=text.replace(key,dict_[value])\n",
    "                            #inline[i].text=text\n",
    "                            \n",
    "#path=\"C:/Users/Shubham/Downloads/\"\n",
    "#doc.save(path+'Fresh_6.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4b094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e411a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=Document(r\"C:/Users/Shubham/Desktop/Data Scientist.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e03c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, this is Shubham Singh. I belong to Delhi. I did my graduation from Delhi University. After that I taught statistics and simultaneously, I was preparing for Data Scientist role as well. Right now I am working as a junior Data Scientist.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc1 in doc.paragraphs:\n",
    "    print(doc1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "407e7b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3736/1041862768.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                         \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhappy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grammar: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeam_settings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                         \u001b[0mdict_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mgrammar_corrected_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrammar_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# Taking model prediction on paragraph text and table text and updating it into a dictionary.\n",
    "dict_={}\n",
    "def grammar_model(sentences):\n",
    "    #Accessing paragraph text\n",
    "    for para in doc.paragraphs:\n",
    "        data=nltk.sent_tokenize(para.text)\n",
    "        for i,sent in enumerate(data):\n",
    "            result=happy.generate_text(\"grammar: \"+sent,args=beam_settings)\n",
    "            if data[i]==result.text:\n",
    "                continue\n",
    "            else:\n",
    "                dict_.update({sent:result.text})\n",
    "    # accessing table text \n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            for cell in row.cells:\n",
    "                for paragraph in cell.paragraphs:\n",
    "                    data=nltk.sent_tokenize(paragraph.text)\n",
    "                    for sent in data:\n",
    "                        result=happy.generate_text(\"grammar: \"+sent,args=beam_settings)\n",
    "                        dict_.update({sent:result.text})\n",
    "grammar_corrected_sentences=grammar_model(sentences)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c21c726b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "doc=Document(r\"C:/Users/Shubham/Desktop/Data Scientist.docx\")\n",
    "\n",
    "# Updating grammar corrected sentences in original document paragraph.\n",
    "for p in doc.paragraphs:\n",
    "    inline = p.runs\n",
    "    for i in range(len(inline)):\n",
    "        text = inline[i].text\n",
    "        for key in dict_.keys():\n",
    "            if key in text:\n",
    "                text=text.replace(key,dict_[key])\n",
    "                inline[i].text = text\n",
    "#Updating grammar corrected sentences in original document tables.                   \n",
    "for table in doc.tables:\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            for paragraph in cell.paragraphs:\n",
    "                inline=paragraph.runs\n",
    "                for i in range(len(inline)):\n",
    "                    text=inline[i].text\n",
    "                    for key in dict_.keys():\n",
    "                        if key in text:\n",
    "                            text=text.replace(key,dict_[key])\n",
    "                            inline[i].text=text\n",
    "\n",
    "path=\"C:/Users/Shubham/Downloads/\"\n",
    "doc.save(path+'Fresh_12.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd59713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from docx import Document\n",
    "#doc=Document(r\"C:/Users/Shubham/Desktop/Data Scientist.docx\")\n",
    "\n",
    "#class Sentence_Replacement:\n",
    "    #def __init__(self,data):\n",
    "        #self.data=data\n",
    "        \n",
    "    #def para_replacement(self):\n",
    "        #for p in self.data.paragraphs:\n",
    "            #inline = p.runs\n",
    "            #for i in range(len(inline)):\n",
    "                #text = inline[i].text\n",
    "                #for key in dict_.keys():\n",
    "                    #if key in text:\n",
    "                        #text=text.replace(key,dict_[key])\n",
    "                        #inline[i].text = text\n",
    "                        \n",
    "    #def table_replacement(self):\n",
    "        #for table in self.data.tables:\n",
    "            #for row in table.rows:\n",
    "                #for cell in row.cells:\n",
    "                    #for paragraph in cell.paragraphs:\n",
    "                        #inline=paragraph.runs\n",
    "                        #for i in range(len(inline)):\n",
    "                            #text=inline[i].text\n",
    "                            #for key,value in dict_.items():\n",
    "                                #if key in text:\n",
    "                                    #text=text.replace(key,dict_[key])\n",
    "                                    #inline[i].text=text\n",
    "                        \n",
    "                        \n",
    "#res=Sentence_Replacement(doc)\n",
    "#print(res.para_replacement())\n",
    "#print(res.table_replacement()) \n",
    "\n",
    "#path=\"C:/Users/Shubham/Downloads/\"\n",
    "#doc.save(path+'Fresh_15.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8439f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_corrected_output=\"\"\"Hi, this is Shubham Singh. I belong to Delhi. I did my graduation from Delhi University. After that I taught statistics and simultaneously, I was preparing for Data Scientist role as well. Right now I am working as a junior Data Scientist.\n",
    "\n",
    "\n",
    "Data Scienc\n",
    "Statisics\n",
    "Data Scienc leverages the power of Data, Mathematics, and computer programming.\n",
    "Statisics is a branch of mathematics which is used to built predictive model.\n",
    "Data Scienc is much more advance.\n",
    "We can use Statisics for smaller data set.\n",
    "Data Scienc evolved recently.\n",
    "Statisics is in used for a long time.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9295ab91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi, this is Shubham Singh.',\n",
       " 'I belong to Delhi.',\n",
       " 'I did my graduation from Delhi University.',\n",
       " 'After that I taught statistics and simultaneously, I was preparing for Data Scientist role as well.',\n",
       " 'Right now I am working as a junior Data Scientist.',\n",
       " 'Data Scienc\\nStatisics\\nData Scienc leverages the power of Data, Mathematics, and computer programming.',\n",
       " 'Statisics is a branch of mathematics which is used to built predictive model.',\n",
       " 'Data Scienc is much more advance.',\n",
       " 'We can use Statisics for smaller data set.',\n",
       " 'Data Scienc evolved recently.',\n",
       " 'Statisics is in used for a long time.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentences=nltk.sent_tokenize(spell_corrected_output)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cfb2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data=\"\"\"Hi, this is Shubham Singh. I belong to Delhi. I did my graduation from Delhi University. After that I taught statistics and simultaneously, I was preparing for Data Scientist role as well. Right now I am working as a junior Data Scientist.\n",
    "\n",
    "\n",
    "Data Scienc\n",
    "Statisics\n",
    "Data Scienc leverages the power of Data, Mathematics, and computer programming.\n",
    "Statisics is a branch of mathematics which is used to built predictive model.\n",
    "Data Scienc is much more advance.\n",
    "We can use Statisics for smaller data set.\n",
    "Data Scienc evolved recently.\n",
    "Statisics is in used for a long time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb0dd26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi, this is Shubham Singh.',\n",
       " 'I belong to Delhi.',\n",
       " 'I did my graduation from Delhi University.',\n",
       " 'After that I taught statistics and simultaneously, I was preparing for Data Scientist role as well.',\n",
       " 'Right now I am working as a junior Data Scientist.',\n",
       " 'Data Scienc\\nStatisics\\nData Scienc leverages the power of Data, Mathematics, and computer programming.',\n",
       " 'Statisics is a branch of mathematics which is used to built predictive model.',\n",
       " 'Data Scienc is much more advance.',\n",
       " 'We can use Statisics for smaller data set.',\n",
       " 'Data Scienc evolved recently.',\n",
       " 'Statisics is in used for a long time.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_sent=nltk.sent_tokenize(original_data)\n",
    "orig_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15f36a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_={}\n",
    "for i,(orig_sent,spell_sent) in enumerate(zip(orig_sent,sentences)):\n",
    "    res=happy.generate_text(\"grammar: \"+spell_sent,args=beam_settings)\n",
    "    if sentences[i]==res.text:\n",
    "        continue\n",
    "    else:\n",
    "        dict_.update({orig_sent:res.text})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "199b7084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I belong to Delhi.': 'i belong to delhi',\n",
       " 'I did my graduation from Delhi University.': 'i did my graduation from delhi university',\n",
       " 'After that I taught statistics and simultaneously, I was preparing for Data Scientist role as well.': 'after that i taught statistics and simultaneously i was preparing for a data scientist role as well',\n",
       " 'Right now I am working as a junior Data Scientist.': 'Right now I am working as a junior data scientist',\n",
       " 'Data Scienc\\nStatisics\\nData Scienc leverages the power of Data, Mathematics, and computer programming.': 'Data Scienc Statisics Data Scienc leverages the power of Data Mathematics and computer programming',\n",
       " 'Statisics is a branch of mathematics which is used to built predictive model.': 'Statisics is a branch of mathematics which is used to build a predictive model.',\n",
       " 'We can use Statisics for smaller data set.': 'we can use statisics for a smaller data set.',\n",
       " 'Statisics is in used for a long time.': 'Statisics is used for a long time.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca0403db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "doc=Document(r\"C:/Users/Shubham/Desktop/Data Scientist.docx\")\n",
    "\n",
    "# Updating grammar corrected sentences in original document paragraph.\n",
    "for p in doc.paragraphs:\n",
    "    inline = p.runs\n",
    "    for i in range(len(inline)):\n",
    "        text = inline[i].text\n",
    "        for key in dict_.keys():\n",
    "            if key in text:\n",
    "                text=text.replace(key,dict_[key])\n",
    "                inline[i].text = text\n",
    "#Updating grammar corrected sentences in original document tables.                   \n",
    "for table in doc.tables:\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            for paragraph in cell.paragraphs:\n",
    "                inline=paragraph.runs\n",
    "                for i in range(len(inline)):\n",
    "                    text=inline[i].text\n",
    "                    for key in dict_.keys():\n",
    "                        if key in text:\n",
    "                            text=text.replace(key,dict_[value])\n",
    "                            inline[i].text=text\n",
    "\n",
    "path=\"C:/Users/Shubham/Downloads/\"\n",
    "doc.save(path+'Fresh_14.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caffdb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
